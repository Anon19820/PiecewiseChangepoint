---
title: "Gibbs sampling for piecewise constant models"
author: "Philip Cooney"
date: "13 December 2019"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    includes:
      in_header: preamble-latex.tex
  bookdown::word_document2: default
  bookdown::html_document2: default
link-citations: yes
bibliography:
- References_Gibs.bib

---

# Piecewise Exponential model

One of the most convenient and popular models for semiparametric survival analysis is the piecewise constant hazard model. Let the survival times $y_i$ be distributed as piecewise exponential random variables. Within this specification, time is partitioned into $J$ intervals with changepoints $0 = \tau_0 < \tau_1 < \dots < \tau_j = \infty$ and the hazard is _constant_ within each interval, so that $\lambda_0(t) = \lambda_j$ for $y$ in $[\tau_{j-1}, \tau_j)$.

The likelihood of the piecewise exponential model (conditional on the changepoints) can be formulated as follows:

Let $ \text{D} = (n,y,v)$ denote the observed data, where $\mathbf{y} = (y_1,y_2,\dots,y_n)'$, $\mathbf{v} = (v_1,v_2,\dots,v_n)'$ with $v_i = 1$ if the $i^{\text{th}}$ subject failed and 0 otherwise. Letting $\mathbf{\lambda} = (\lambda_1,\lambda_2,\dots,\lambda_J)'$, we can write the likelihood function of $\lambda$ for the $n$ subjects as:

$$L(\lambda|D) = \prod_{i=1}^n \prod_{j=1}^J \lambda_j^{\delta_{ij}v_i} exp\bigg\{-\delta_{ij} \bigg[\lambda_j (y_i - \tau_{j-1}) + \sum_{g=1}^{j-1} \lambda_g(\tau_g - \tau_{g-1}) \bigg] \bigg\}$$
where $\delta_{ij} = 1$ if the $i^{\text{th}}$ subject failed or was censored in the  $j^{\text{th}}$ interval.


As a consequence of the constant hazards, the times within each interval are distributed as exponential random variables. The pdf of an exponential is:
$$\mathcal{E}(X|\lambda) = \lambda e^{-\theta X}, \quad 0 \leq X, 0 < \lambda$$

A convient prior for the exponential hazard ($\lambda$) is the gamma distribution. Because the exponential is a special case of the gamma distribution where the shape parameter is fixed at one, it is straightforward to prove that the gamma is congugate to the exponential.   

The gamma PDF is:
$$f(\lambda | \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\beta \lambda}$$
Suppose we now observe $x_1, x_2, x_n \sim \text{iid} \;  \mathcal{E}(X|\lambda)$ and produce the likelihood function:


$$L(\lambda | \text{x}) = \prod_{i=1}^n \lambda e^{-\lambda x_i} = \lambda^n exp \bigg[-\lambda \sum_{i = 1}^n x_i \bigg] $$
Note that $\sum_{i=1}^n x_i$ is a sufficient statistic for $\lambda$. The posterior distribution is as follows:

\begin{align*}
\pi(\lambda| \text{x}) &\propto L(\lambda | \text{x})p(\lambda)\\
&= \lambda^n exp \bigg[ -\lambda \sum_{i=1}^n x_i \bigg]\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\beta \lambda}\\
&\propto \lambda^{(\alpha +n)-1} exp \bigg[ - \lambda \bigg( \sum_{i=1}^n x_i + \beta \bigg) \bigg]\\
\end{align*}

It is easy to see that this is the kernel of a $\text{Gamma}(\alpha + n , \sum x_i + \beta)$, and therefore the gamma distribution is shown to be conjugate to the exponential likelihood function. Therefore, conditional on the changepoints we can obtain a posterior sample of the hazard.


#Gibbs Sampler

##One-changepoint model

Because we can compute the conditonal distribution of the lambda's in each interval and  compute the likelihood across all potential changepoints (by restricting changepoints to be event times), we can use Gibbs sampling to obtain the marginal distribution for each variable by iteratively conditioning on interim values of the other parameters in a continuing cycle.

Assume the following model with independent priors

\begin{align*}
   k &\sim \text{Uniform} \{1,2,\cdots,n\}\\
   \lambda_{1} &\sim \text{Gamma}(\alpha_1, \beta_1)\\
   \lambda_{2} &\sim \text{Gamma}(\alpha_2, \beta_2)\\
\end{align*}

where $\alpha_1$, $\beta_1$, $\alpha_2$ and $\beta_2$ are independently distributed as a $\text{Gamma}(0.5, 5)$ random variables. These priors are non-informative as the mean of this distribution is 0.1 $E[X] = \frac{\alpha}{\beta}$.

Using the fact that the gamma distribution is conjugate prior to the exponential; we obtain the following:

\begin{align*}
\lambda_{1}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_1 +d_{[\tau_0 - \tau_1)}, \beta_1 + \sum_{i=1}^{[\tau_0 - \tau_1)} t)\\
\lambda_{2}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_2 +d_{[\tau_1 - \infty)}, \beta_2 + \sum_{i=1}^{[\tau_1 - \infty)} t)\\                  
\end{align*} 

where k is the location of the changepoint (represented in terms of the number of deaths that have occured), $\tau$ is time of the changepoint, d are the deaths in the interval and t is the exposure time for all observations within the interval. The posterior density of the changepoint $k$ is 

\begin{equation}
f(k|D,\lambda,\alpha, \beta) = \frac{L(D;k,\lambda)}{\sum_{j=1}^n L(D;j,\lambda)}
(\#eq:post_change)
\end{equation}

which is the likelihood of the piecewise exponential model with a changepoint at k, divided by all possible changepoint models.

The model proceeds as follows:

1. Initialize k by random draw from 2:(n-1) events
2. For each interation, indexed $m = 1,2, \dots$ repeat:

  1. For the current value of k, define the number of events and the exposure time within each interval.
  2. Using these k values, compute the number of deaths and exposure time within each inteval.
  3. Sample  $\lambda \sim  \text{Gamma}(\alpha + d, \beta + t)$ for each inteval where d is the number of events and t is the exposure time in the interval.
  4. Sample $\alpha \; \& \; \beta \sim \text{Gamma}(0.01, \lambda + 0.01)$
  5. Evaluate the likelihood for all potential changepoints from $2:n-1$
  6. Generate  a new changepoint from the multinomial distribution defined by Equation 1 using the updated values of $\lambda_1$ and $\lambda_2$.
  7. Increment m


###Validation

From the code; I have validated that the various functions. I first validate that the function that is used to simulate piecewise exponential times works as expected Figure (\@ref(fig:piecewise-expo-haz)). 

**Not shown:** I then test that the loglikelihood of my "stripped" down version of phreg produces the same log-likelihood (at the MLE's). I then ensure that my other function which computes the log-likelihood for a given change point and hazard is correct. I do this by inputting the MLE hazards and obtaining the same log-likelihood as full phreg model. 

The algorithm seems to preform reasonably well, 500 iterations are completed in <4 mins with 100 discarded for burn-in. Figure \@ref(fig:gibbs-haz) presents the posterior samples of the hazards (in red), the mean of these posterior samples (black) and the "true" underlying hazards. The "true" hazards of 0.3 and 0.5 and timepoint of 3 years are recovered with some discrepancy expected because 300 exponential times were simulated and therefore the observed means maybe subject to sample variation. Table \@ref(tab:summary-tab) presents summary statistics for the changepont and hazards.  Figure \@ref(fig:piecewise-expo-surv) shows the Kaplan-Meier survival of the simulated observations versus the mean posterior survival (red). The mean posterior survival is a reasonable estimator of the Kaplan-Meier survival function and remains with the 95% confidence intervals across the timepoints.


```{r, include = FALSE}

setwd("C:/Users/phili/OneDrive/PhD/R codes")
source("Functions Gibbs Markdown.R")


###### Validation of piecewise exponential functions
n <- 300
rate <- c(0.2, .5,0.25)
ratemat <- matrix(rep(rate, n/2), nrow = n, 
                  ncol = 3, byrow = TRUE)
t1 <- 1.5
t2 <-3

t <- c(0, t1, t2) 
ptm <- proc.time()
samp <- hesim::rpwexp(n, ratemat, t)
proc.time() - ptm
summary(samp)
event <- rep(1,n)

plot(survfit(Surv(samp,event)~1))

samp <- samp[order(samp)]


df <- data.frame(time =samp, status =event , enter = 0)
res.out  <- exposure_death(df, changepoint = c(t1,t2))
res.out[,1]/res.out[,2]


#########

```



```{r, eval = FALSE, include = FALSE}

#Verify the piecewise exponential functions

df_test <- data.frame(time = samp, status = 1 , enter = 0)
df_test <- df_test[order(df_test$time),]


changepoint <- summary(df_test$time)["Median"]
piecewise_loglik(df_test, changepoint = changepoint)

fit <- phreg(Surv(enter, time, status) ~ 1, data = df_test, dist = "pch", cuts = changepoint)
fit$loglik[1]
fit$hazards
piecewise_loglik(df_test, changepoint = changepoint, method = "Not ML", lambda = fit$hazards)

```

```{r, cache = TRUE, results = 'hide'}

#Consider a 1 changepoint model
num.breaks <- 1
rate <- c(.3, 0.5)
# number of exponentially distributed observations
n <- 300
ratemat <- matrix(rep(rate, n/2), nrow = n,
                  ncol = 2, byrow = TRUE)

t <- c(0, 3)
samp <- rpwexp(n, ratemat, t)
event <- rep(1,n)

df <- data.frame(time =samp, status =event , enter = 0)
df <- df[order(df$time),]

#redunant for now but will be required when censored observations are included
event.df <- df[which(df$status == 1),]

n <- sum(df$status) #count number of events
m <- 500#length of the chain
lambda1 <- lambda2 <- changepoint <- k <- rep(NA,m) #Initialize lambda 1, lambda 2


k[1] <- sample(2:(n-1),1) #sample k 2:(n-1) 

#Hyperparameters 
a1 <- a2 <- b1 <- b2 <- 1

#run the Gibbs sampler 

for (i in 2:m) {
  kt <- k[i-1]
  changepoint[i] <- event.df[kt,"time"]
  #Indentify  number of events and exposure time for the given changepoint
  #I could use k directly as the number of events but this wouldn't generalize to more 
  #changepoints
  res_array <- exposure_death(df, changepoint = changepoint[i])
  #generate lambda1
  lambda1[i] <- rgamma(1, shape = a1 + res_array[1,1], rate = b1 + res_array[1,2])
  #generate lambda2
  lambda2[i] <- rgamma(1, shape = a2 + res_array[2,1], rate = b2 + res_array[2,2])
  #generate a1, a2, b1 and b2
  a1 <- b1 <- rgamma(1, shape = .01, rate = 0.01)
  a2 <- b2 <- rgamma(1, shape = .01, rate = 0.01)
  
  LL <-sapply(event.df[-c(1,n),1], FUN = piecewise_loglik, df = df, method = "Not ML" ,
             lambda = c(lambda1[i],lambda2[i]) )
  
  #Use Brobdingnag package because the numbers are so small that R doesn't compute them
  L <- exp(as.brob(LL))
  L <- as.numeric(L /Brobdingnag::sum(L))
  L <- L / sum(L)
  #generate k from discrete distribution L on 2:n-1
  k[i] <- sample(2:(n-1), prob=L, size=1)
}


```

```{r, include = FALSE}
burn_in <- 100
df_summary <- data.frame(changepoint = as.vector(changepoint[-c(1:burn_in)]),
                         lambda1 = lambda1[-c(1:burn_in)],
                         lambda2 = lambda2[-c(1:burn_in)])

rounded_df_summary <- round_df(df_summary, digits = 2)

```

```{r summary-tab, echo= FALSE, warning = FALSE}
knitr::kable(summary(rounded_df_summary), digits = 2, caption = 'Summary statistics for the changepoints and hazards') %>%
kable_styling(latex_options = c("striped", "hold_position"))

```

```{r, include = FALSE}
#end_time <- Sys.time()

#end_time - start_time

changepoint <- changepoint[-c(1:burn_in)]
initial.time <- rep(0, length(changepoint))
lambda1 <- lambda1[-c(1:burn_in)]
lambda2 <- lambda2[-c(1:burn_in)]


df.plot <- data.frame( timepoints =c(rbind( initial.time,changepoint, max(samp))),
                       hazards = c(rbind(lambda1,lambda2,lambda2)),
                       id = rep(1:length(changepoint),each =3))

df.mean <- data.frame( timepoints =c(rbind( initial.time,mean(changepoint), max(samp))),
                       hazards = c(rbind(mean(lambda1),mean(lambda2),mean(lambda2))),
                       id = rep(1:1,each =3))

df.true <- data.frame(timepoints = c(t, max(samp) ),
                      hazards = c(rate, rate[length(rate)]))
```

```{r gibbs-haz, echo = FALSE, results = 'hide', fig.cap=  'Posterior samples (red), Underlying hazard (green), Posterior mean hazard (black)', warning = FALSE}

ggplot(df.plot, aes(timepoints, hazards))+ 
  geom_step(aes(group = id), linetype = "dashed", alpha = 0.075, colour = "red")+
  geom_step(data = df.mean)+
  geom_step(data = df.true, colour = "green")+
  ylim(0, .65)
```


```{r, cache = FALSE, include = FALSE}

time <- seq(from = 0, to  = 12, by = 0.1)
t.len <- length(time)
St <- array(NA, dim = c(length(time),nrow(df_summary)))


for(i in 1:nrow(df_summary)){
  for (j in 1:t.len){
    if(time[j] < df_summary[i,1]){
      St[j,i] <-  exp(-df_summary[i,2]*time[j]) 
      
    }else{
      time.diff <- time[j] -df_summary[i,1]
      St[j,i] <- exp(-df_summary[i,1]*df_summary[i,2])*exp(-df_summary[i,3]*time.diff) 
    }
  
  }
}

KM.fit <- survfit(Surv(time,status)~1, data = df)
```


```{r piecewise-expo-surv, fig.cap = 'Kaplan Meier vs mean posterior survival (red)',echo = FALSE, results = 'hide', out.height= '80%', fig.asp= .75}
plot(KM.fit, xlab = "Time", ylab = "Survival probability")
lines(time, rowMeans(St), col = "red")
title("Survival plot (KM and mean predicted Survival)")
points(y = exp(-mean(df_summary[,1])*mean(df_summary[,2])),
       x = mean(df_summary[,1], na.rm = T), pch = 23, bg = "green")

```



##Multiple-changepoint model

It is theoretically straightforward to consider models with more than 1 changepoint. Letting n be the number of changepoints, k becomes the ordered vector $\mathbf{k} = (k_1,k_2,\dots,k_n)'$, while the hazards ($\lambda$) and hyperparameters ($\alpha$,$\beta$) become vectors with lenght n + 1. 

The model is intialized by samplying n changepoints (without replacement). The elements of the $\alpha$ & $\beta$ vectors are each independent draws from a $\text{Gamma}(0.5, 5)$ distribution.  

Based on the changepoints the elements of the $\lambda$ vector will be sampled as below. 

\begin{align*}
\lambda_{1}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_1 +d_{[\tau_0 - \tau_1)}, \beta_1 + \sum_{i=1}^{[\tau_0 - \tau_1)} t)\\
\lambda_{2}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_2 +d_{[\tau_1 - \infty)}, \beta_2 + \sum_{i=1}^{[\tau_1 - \tau_2)} t)\\ 
\ .\\
\ . \\
\lambda_{n+1}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_{n+1} +d_{[\tau_{n} - \infty)}, \beta_{n+1} + \sum_{i=1}^{[\tau_{n} - \infty)} t)\\ 
\end{align*} 

The posterior density of the first changepoint is calculated by evaluating the model likelihood from changepoints $2$ to $k_{2-1}$ conditional on the the changepoint $k_2$ and $\lambda_2$ (See \@ref(eq:post_change)). Based on the newly sampled changepoint, the hazards are updated and posterior density of the second changepoint is calculated by evaluating the likelihood $k_{1+1}$ to $k_{3-1}$. The second changepoint is sampled from this posterior and the process continues until all the changepoints have been evaluated. A _DAG_ (directed acyclic graph) is presented in Figure \@ref(fig:DAG). For each changepoint (from 1:n) an $\alpha$, $\beta$ and changepoint location k are sampled. The number of events (d) and the exposure time for each interval (t) are determinstic functions of the changepoint location k and their relationship to k is denoted by black double arrows. This notation is consistent with illustrations used in [@Lunn.2013].

```{r DAG, fig.cap = 'DAG for the multiple changepoint model', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

knitr::include_graphics("C:/Users/phili/OneDrive/PhD/Plots/DAG-multiple.png")
```

```{r, cache = TRUE, results = 'hide' }
#Consider a x changepoint model
num.breaks <- 2
rate <- c(.3, 0.5, 0.2)
# number of exponentially distributed observations
n <- 500
ratemat <- matrix(rep(rate, n/2), nrow = n,
                  ncol = num.breaks+1, byrow = TRUE)

t <- c(0, 3,5)
samp <- rpwexp(n, ratemat, t)
event <- rep(1,n)

df <- data.frame(time =samp, status =event , enter = 0)
df <- df[order(df$time),]

#redunant for now but will be required when censored observations are included
event.df <- unique(df[which(df$status == 1),])

n <- sum(df$status) #count number of unique event times
n.vec <- 1:n
m <- 500 #length of the chain
#Create vectors to store the changepoints and lambdas
changepoint <- k <- array(NA, dim = c(m, num.breaks)) 
lambda <- array(NA, dim = c(m, num.breaks+1))
k[1,] <- sample(2:(n-1), num.breaks, replace = FALSE) #sample k 2:(n-1) 
k[1,] <- k[1,][order(k[1,])]

#Initial alpha and beta Hyperparameters 
beta_array <- alpha_array <- array(rgamma(m*(num.breaks + 1), shape = 0.01, rate = 0.01),
                                   dim = c(m, (num.breaks+1)))
             
#run the Gibbs sampler 
for (i in 2:m) {
  kt <- k[i-1,]
  for(j in 1:(num.breaks)){
    
  changepoint[i,] <- event.df[kt,"time"]
  #Indentify  number of events and exposure time for the given changepoints
  res_array <- exposure_death(df, changepoint = changepoint[i,])
  #generate lambdas for all intervals
 
    for(q in 1:(num.breaks+1)){
      lambda[i,q] <-   rgamma(1, shape = alpha_array[i,q] + res_array[q,1], 
                              rate = beta_array[i,q] + res_array[q,2])} 
  
  #Compute the likelihood for the allowable changepoints
  
  #Get the interval of observations to test on 
  if(num.breaks == 1){
    evals <- n.vec[-c(1,n)]
    interval.df <- event.df[evals,1]
    eval_lambda <- 1:2
  }else if(num.breaks != 1 & j == num.breaks){ #final interval
    evals <- c((kt[j-1]+1):(n-1))
    interval.df <- cbind(event.df[kt[j-1],1],event.df[evals,1])
    eval_lambda <- (ncol(lambda)-2):ncol(lambda)
  }else if(num.breaks !=1 & j == 1){ #first interval
    evals <- c(2:(kt[j+1]-1))
    interval.df <- cbind(event.df[evals,1],event.df[kt[j+1],1])
    eval_lambda <- 1:3
  }else{ #middle interval 
    evals <- c((kt[j-1]+1):(kt[j+1]-1))
    interval.df <- cbind(event.df[kt[j-1],1],
                         event.df[evals,1],
                         event.df[kt[j+1],1])
    eval_lambda <- (j-1):(j+2)
  } 
  
  LL <-apply(interval.df,1,
               FUN = piecewise_loglik, df = df, method = "Not ML" ,
               lambda = lambda[i,eval_lambda]) 
  #Use Brobdingnag package because the numbers are so small that R doesn't compute them
  L <- exp(as.brob(LL))
  L <- as.numeric(L /Brobdingnag::sum(L))
  L <- L / sum(L)
  
  kt[j] <- sample(evals, prob=L, size=1)
  }
  
   k[i,] <- kt
   #Final evaluation of hazards
   for(q in 1:(num.breaks+1)){
     lambda[i,q] <-   rgamma(1, shape = alpha_array[i,q] + res_array[q,1], 
                             rate = beta_array[i,q] + res_array[q,2])} 
   
}
```


```{r, echo = FALSE, results = 'hide'}

#Processing the results

burn_in <- 50
changepoint_names <- rep(NA, num.breaks)
lambda_names <- rep(NA, num.breaks+1)
for(i in 1:num.breaks){
  changepoint_names[i] <- paste0("changepoint_", i)
  } 
for(i in 1:(num.breaks+1)){
  lambda_names[i] <- paste0("lambda_", i)
} 
names_vector <- c(changepoint_names,lambda_names)

output_df <- cbind(changepoint, lambda)
output_df <- output_df[-c(1:burn_in),]
colnames(output_df) <- names_vector


```

```{r, echo = FALSE, results = 'hide'}

#Ploting the results

samp.plot <- df_hazard_plot(df = output_df, time.vec = samp, num.breaks = num.breaks)


df.summary <- df_hazard_plot(df = data.frame(t(apply(output_df,2,mean))),
               time.vec = quantile(samp,0.6), num.breaks = num.breaks)

ggplot(samp.plot, aes(timepoints, hazards))+ 
  geom_step(aes(group = id), linetype = "dashed", alpha = 0.075, colour = "red")+
  geom_step(data = df.summary)+
  ylim(c(0,1))+
  scale_x_continuous(breaks = seq(0, 40, by = 5))+
  annotate(geom="segment", x=seq(0,40,1), xend = seq(0,40,1),
           y=0, yend= 0.01)
rounded_output_df <- round_df(output_df,2)

```


```{r summary-tab-multi, echo= FALSE, warning = FALSE}
knitr::kable(summary(rounded_output_df), digits = 2, caption = 'Summary statistics for the changepoints and hazards') %>%
kable_styling(latex_options = c("striped", "hold_position"))

```


```{r, echo = FALSE, results = 'hide'}


time <- c(seq(from = 0, to  = max(samp), by = 0.1))
time.break <- df.summary[-c(1,nrow(df.summary)),
                         grep("timepoints", colnames(df.summary))]
t.len <- length(time)

St <- array(NA, dim = c(length(time),nrow(output_df)))
index <- grep("changepoint", colnames(output_df))

changepoint_df <- cbind(0,output_df[,index])
time.interval_df <- t(apply(changepoint_df,1,diff))

index2 <- grep("lambda", colnames(output_df))
lambda_df <- output_df[,index2]

cum_haz_df <- t(apply(time.interval_df*output_df[,head(index2,-1)], 1,cumsum))

surv_df <-  cbind(1,exp(-cum_haz_df))

break.points.Surv <- data.frame(time = time.break, Survival =  colMeans(surv_df[,-1]))

for(i in 1:nrow(output_df)){
  for (j in 1:t.len){
    if(max(which(time[j] >= changepoint_df[i,]))==1){
      St[j,i] <-  exp(-lambda_df[i,1]*time[j]) 
      
    }else{
      index3 <- max(which(time[j] >= changepoint_df[i,]))
      
      time.diff <- time[j] -changepoint_df[i,index3]
      St[j,i] <- surv_df[i,index3]*exp(-lambda_df[i,index3]*time.diff) 
    }
    
  }
}


St_mean <- data.frame(time = time, Survival = rowMeans(St))
result.km <- survfit(Surv(time,status)~1, data = df)

km.data <- data.frame(cbind(result.km[[c("time")]],result.km[[c("surv")]], result.km[[c("upper")]],result.km[[c("lower")]]))
colnames(km.data) <- c("time", "survival", "upper", "lower")


Surv.plot <- data.frame(Survival = c(unlist(St)), 
                      time = rep(time,nrow(output_df)),
                      id = rep(1:nrow(output_df), each = length(time)))


```

```{r, piecewise-expo-surv-mult, fig.cap = 'Kaplan Meier vs mean posterior survival (blue), average breakpoint (green)',echo = FALSE, results = 'hide', out.height= '80%', fig.asp= .75}
ggplot(data = Surv.plot,  aes(x = time, y = Survival))+
  geom_line(aes(group = id),size = 0.1, alpha = 0.025, colour = "red")+
  geom_line(data = St_mean, aes(x = time, y = Survival), colour = "blue")+
  geom_step(data = km.data, aes(x = time, y = survival), inherit.aes = F )+
  geom_step(data = km.data, aes(x = time, y = upper),linetype = "dashed", inherit.aes = F )+
  geom_step(data = km.data, aes(x = time, y = lower), linetype = "dashed", inherit.aes = F )+
  geom_point(data = break.points.Surv, aes(x = time, Survival), shape=23, fill = "green",inherit.aes = F)+
  scale_y_continuous(breaks = seq(0, 1, by = 0.1))+
  annotate(geom="segment", x=seq(0,40,1), xend = seq(0,40,1),
           y=0, yend= 0.01)

#https://github.com/janeshdev/r-dataanalysis-tips/wiki/Add-minor-ticks-to-ggplot2
```

#Model Optimization

The algorithm presented above is computationally intensive due to the fact that the log-likelihood needs to be evaluated for each event time within the interval $k_{(X-1)+1}$ to $k_{(X+1)-1}$. Increasing the number of changepoints adds another changepoint which must evaluated conditional on the other changepoints. In models with a large number of observations and numerous changepoints (i.e. 4), evaluation of iteration is close to 1 second. One could reasonably expect that 5,000 - 10,000 simulations may be required to ensure convergence of the model (although this number may be conservative given that all proposal steps are accepted in a Gibbs sampler) which would diminish the practicality of this method.

The efficency of the method can be enhanced substantially by noting that there is no loss of information in recasting the data as _times between individual observations_ \@ref(fig:Time-between-events). The figure provides the total time experienced by the sample between events with the number of points representing distinct observations. The contribution to log-likelihood for a particular interval is $D \text{log}\lambda - \lambda T $, where D is the number of events and T is the exposure time for that interval. 

When the data is represented like this there is no need to resection the dataset based on the cut points, and the log-likelihood is a trivial summation. The resulting changes made the algorithm over 25 times faster; allowing for large number of simulations to be computed in a reasonable time frame. 

```{r, eval = FALSE}
num.breaks <- 2
rate <- c(.2, .5, 0.3)
# number of exponentially distributed observations
n_event <- 10
ratemat <- matrix(rep(rate, n_event/2), nrow = n_event,
                  ncol = num.breaks+1, byrow = TRUE)

time_break <- c(0, 2, 4)
samp <- hesim::rpwexp(n_event, ratemat, time_break)
samp <- samp[order(samp)]
event <- rep(1,n_event)
df_event <- data.frame(time =samp, status =event , enter = 0)

df <- df_event


plot( y = df_recast(df_event), x =df_event$time, pch = 11, 
      yaxt="none", col = "blue", xlab = "Time of event" , ylab = "Cumulative Time between events")
text(1.5,7, expression(Sigma), col = "red")
text(2,7, "y = Total time", col = "red")
text(1.75,6, "Num. points = Events", col = "blue")
axis(2, seq(0,ceiling(max(df_recast(df_event))),1),las=2, cex.axis=0.8, font=2,col.axis="red")


```


```{r Time-between-events, fig.cap = 'Data recast as time between events', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

knitr::include_graphics("C:/Users/phili/OneDrive/PhD/Plots/Time between events.png")
```

#Model Convergence

Model Convergence was assessed using the Rafferty diagnositic with the quantile (q) = 0.025 and probability (s) = 0.95 with the multiple margins of error (r) tested. If for example we assumed r to be 0.005 this corresponds to requiring that the cumulative distribution function of the 0.025 quantile be estimated to within $\pm$ 0.005 with probability 0.95. This would ensure that our 95% intervals have an actual posterior probability of between 0.94 and 0.96 (Raftery Lewis 1991). It should be noted that the estimates are conservative, so more iterations are suggested than necessary. The diagnostic also provides an assesment of the number of burn-in required.

Chain mixing was assessed by the gelman rubin diagnostic. Gelman and Rubin (1992) propose a general approach to monitoring convergence of MCMC output in which $m > 1$  parallel chains are run with starting values that are overdispersed relative to the posterior distribution. Convergence is diagnosed when the chains have "forgotten"" their initial values, and the output from all chains is indistinguishable. The gelman.diag diagnostic is applied to a single variable from the chain. It is based a comparison of within-chain and between-chain variances, and is similar to a classical analysis of variance.


```{r, cache = TRUE, echo = FALSE}

output <- gibbs.changepoint_chains_optim(df, num.breaks = 2, n.iter = 10000, burn_in = 0, num.chains = 1)
output2 <- gibbs.changepoint_chains_optim(df, num.breaks = 2, n.iter = 500, burn_in = 100, num.chains = 4)
# Run with  5000 chains

```

```{r}
Raferty.Diagnositic.custom(as.matrix(output[1])[[1]][["chain_1"]], r =0.005)
Raferty.Diagnositic.custom(as.matrix(output[1])[[1]][["chain_1"]], r =0.01)
Raferty.Diagnositic.custom(as.matrix(output[1])[[1]][["chain_1"]], r =0.025)
```

```{r, echo = FALSE}
gelman.diag.custom(output2[[1]])
```

Based on this 10,000 simulations were run with only 100 samples discarded.


```{r, cache=TRUE, results = 'hide'}

output_final <- gibbs.changepoint_chains_optim(df, num.breaks = 2, n.iter = 10000, burn_in = 0, num.chains = 1)

```

#Model Validation 
It is possible to fit a piecewise-exponential using the BUGS/JAGS samplers (after a lot of fruitless attempts!). Implementation of the 
model requires use of the counting process formulation (See BUGS book).


```{r}
#Processing of data
N = nrow(df)
T = length(unique(df$time))-1
eps=1.0E-10
obs.t = df$time
ind = df$status
t = unique(df$time)
#Jags model
start1 <-10 # We set uniform priors for the first interval across these ranges
end1 <- 90

start2 <-50 # Uniform prior for the second changepoint
end2 <- 150
p1 <- c(rep(0,start1),rep(1/(end1-start1),(end1-start1)),rep(0,(n-end1)))
p2 <- c(rep(0,start2),rep(1/(end2-start2),(end2-start2)),rep(0,(n-end2)))



Y <- matrix(NA, nrow = N,ncol = T)
dN <- matrix(NA, nrow = N,ncol = T)
dt <- rep(NA, T+1)

for (i in 1:N) {
  for (j in 1:T){
    Y[i,j] <- ifelse(obs.t[i] - t[j] + eps> 0,1,0)
    dN[i,j] <- Y[i,j]*ifelse(t[j+1] - obs.t[i] - eps>0,1,0)*ind[i]
  }
}
dt[1] <- t[1]
for (j in 2:(T+1)) {
  dt[j] <- t[j] - t[j-1]
}

# Model Code

piecewise.expo_optim <- function(){
  
  for (j in 1:T) {
    for (i in 1:N) {
      dN[i,j] ~ dpois(Idt[i,j])
      Idt[i,j] <- Y[i,j]*lam[period[j]]*dt[j]
    }
  }
  cumhaz.treat[1] <- 0
  
  for (j in 2:(T+1)) {
    
    cumhaz.treat[j] <- cumhaz.treat[j-1] + lam[period[j]]*dt[j]
    S.treat[j] <- exp(-cumhaz.treat[j])
  }
  for (j in 1:ndtimes) {
    lam[j] ~ dgamma(0.01, 0.01)
  }
  
  #Code required to make the changepoint a parameter 
  #within the model
  
  for(i in 1:N){
    period[i]  <-  1 + step(i - k1 -eps) + step(i - k2 -eps)
  }
  
  k1 ~dcat(p1[ ])
  k2 ~dcat(p2[ ])
  
}

#Initial functions



data_jags <- list(N = n, T = length(unique(samp))-1,eps=1.0E-10,
             ndtimes = num.breaks+1,
             p1 = p1, p2 = p2, dt =dt, Y = Y, dN = dN)


```


```{r, cache = T}




inits <- function(){list(k1 = sample(10:90, 1),
                         k2 = sample(50:150, 1))}


sim_jag <- R2jags::jags.parallel(data_jags, inits, model.file = piecewise.expo_optim,
            parameters.to.save = c("lam", "S.treat", "k1", "k2", "cumhaz.treat", "period"),
            n.chains = 2, n.iter =2000, n.thin = 1)


```

Add in some results to compare; maybe test convergence

```{r}
1+1

```
Do both approaches provide similar results??


# Data Uncertainty

How does the data preform with changing the number of observations?
I should calculate the hazard observed based on the simulated data;
then compare the theoretical hazards with the observed hazards with the estimated hazards and changepoints.

Next; I should introduce censoring. I should in principle censore with the same piecewise exponential hazards, that would not bias the later samples and a similar fraction would be censored at each interval? May need to check this!

I can plot this against the underlying data and the theoretical distribution. 

I can then plot this against a variety of other distributions exponential, weibull..... Can do these under frequentist method.









#Model Uncertainty

## Evaluation of Pseudo-Marginal Likelihood

As seen in the previous section, Bayesian inference can evaluate the uncertainty in the location of changepoints for a model with a given number of changepoints (i.e. parameter uncertainty). A Bayesian framework can also be used to evaluate the uncertainty associated with the number of changepoints (i.e. model uncertainty). As discussed in [@Jackson.2010] the utility function $\boldsymbol U(\cdot)$ can be defined as the posterior predictive likelihood for $\boldsymbol y$, i.e. the likelihood integrated over the posterior distribution of the model parameters $\Theta$ as

$$\mathbf U_P(\mathbf y|\mathbf x,\mathbf M_k) = \int f(\mathbf y |\theta,\mathbf M_k ) \pi(\theta|\mathbf x,\mathbf M_k)d\theta$$

The expectation of this predictive utility for a replicate data set can be estimated, using only the sample data, by a cross-validatory predictive density termed PML.

$$f_P(\mathbf x|\mathbf M_k = \prod_if(x_i|))$$

It differs in aim from the marginal likelihood in expression (5 addin!), assessing predictive ability rather than fidelity to the data. Gefland adn Dey 1994 (add in reference) described an importance sampling method for estimating the PML based on a single MCMC model fit, which avoids the need to refit the model with each observation exclued in turn. (For ease of notation in this section, the dependence on the model $M_k$ is omitted.)

The full data posterio density $\pi (\theta| \mathbf x)$ is used as a proposal distribution to approximate the leave-one-out posterior density $\pi (\theta| \mathbf x_{(i)})$. Given an MCMC sample $\theta_1, \dots, \theta_N$ from the posterior of of \theta, the importance weights are then $w_{ir} = \pi(\theta_r| \mathbf x_{(i)})) / \pi(\theta_r| \mathbf x)  \propto 1/f(x_i | \theta_r)$, and the importance sampling estimate of $f(x_i | \mathbf x_{(i)})$ over the posterior sample:

\begin(align*)
f(x_i | \mathbf x_{(i)}) &\approx \sum_r w_{ir} f(x_i | \mathbf x_{(i)})/ \sum_r w_{ir} \\
&= N/ \sum_r \frac{1}{f(x_i | \theta_r)} \\
\end(align*)

#Calculating best predictive model

To avoid the computation expense of refitting models to calculate the model selection probabilities, the Bayesian bootstrap method described by Vehtari and Lampine (2002) was used. Instead of sampling with replacement from $\mathbf x$, the Bayesian bootstrap samples sets of probabilities $q_i$ that the random variable $X$ underlying the data takes the value of each sample point $x_i$. In one bootstrap iteration, samples $q_i^{(\text{rep})}$ of q_i are drawn from a "flat" Dirichlet distrution with all parameters 1. This is the psoterior distrution of _the disribution of X_, conditionally on the sample $\mathbf x$ and an improper prior (Rubin , 1981). The bootstrap replicate of the sample statistic is then computed by using the original data $\mathbf x$ with weights of $q_i^{(\text{rep})}$.

For the log(PML) example, the log-predictive-ordinates for each point $x_i$ is:

$$log{f_P(\mathbf x | M_k)} = \sum_{i= 1}^n log{f(x_i | \mathbf x_{(i)}, M_k)}$$ 

where $n$ is the sample size. The Bayesian bootstrap replicate of the log(PML) is then

$$log{f_P(\mathbf x | M_k)}^{\text(rep)} = n\sum_{i= 1}^n q_i^{\text(rep)}log{f(x_i | \mathbf x_{(i)}, M_k)}$$ 

```{r, cache = TRUE,echo = FALSE, results = 'hide' }

num.breaks <- 2
rate <- c(.3, 0.5, 0.2)
# number of exponentially distributed observations
n <- 500
ratemat <- matrix(rep(rate, n/2), nrow = n,
                  ncol = num.breaks+1, byrow = TRUE)

t <- c(0, 3,5)
samp <- rpwexp(n, ratemat, t)
event <- rep(1,n)

df <- data.frame(time =samp, status =event , enter = 0)

iters <- 2000
burn_in <- iters*.1

output.list_1 <- gibbs.changepoint_chains(df = df, num.breaks = 1, n.iter = iters, burn_in = burn_in)
output.list_2 <- gibbs.changepoint_chains(df = df, num.breaks = 2, n.iter = iters, burn_in = burn_in)
output.list_3 <- gibbs.changepoint_chains(df = df, num.breaks = 3, n.iter = iters, burn_in = burn_in)
output.list_4 <- gibbs.changepoint_chains(df = df, num.breaks = 4, n.iter = iters, burn_in = burn_in)

boot.samps <- 5000
alpha <- c(rep(1,nrow(df)))
weight_vec <- t(rdirichlet(boot.samps, alpha))

PML_1 <- PML.calc(origin.df = df, output_df = output.list_1[[4]],weights = weight_vec,
                  num.breaks = 1)
PML_2 <- PML.calc(origin.df = df, output_df = output.list_2[[4]], weights = weight_vec,
                  num.breaks = 2)
PML_3 <- PML.calc(origin.df = df, output_df = output.list_3[[4]], weights = weight_vec,
                  num.breaks = 3)
PML_4 <- PML.calc(origin.df = df, output_df = output.list_4[[4]], weights = weight_vec,
                  num.breaks = 4)


```




```{r, echo = TRUE, eval = FALSE}

round(table(apply(cbind(PML_1,PML_2,PML_3,PML_4),1, which.max))/boot.samps, digits = 2)

```
