---
title: "Gibbs sampling for piecewise constant models"
author: "Philip Cooney"
date: "13 December 2019"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    includes:
      in_header: preamble-latex.tex
  bookdown::word_document2: default
  bookdown::html_document2: default
link-citations: yes
bibliography:
- References_Gibs.bib

---

# Piecewise Exponential model

One of the most convenient and popular models for semiparametric survival analysis is the piecewise constant hazard model. Let the survival times $y_i$ be distributed as piecewise exponential random variables. Within this specification, time is partitioned into $J$ intervals with changepoints $0 = \tau_0 < \tau_1 < \dots < \tau_j = \infty$ and the hazard is _constant_ within each interval, so that $\lambda_0(t) = \lambda_j$ for $y$ in $[\tau_{j-1}, \tau_j)$.

The likelihood of the piecewise exponential model (conditional on the changepoints) can be formulated as follows:

Let $\text{D} = (n,y,v)$ denote the observed data, where $\mathbf{y} = (y_1,y_2,\dots,y_n)'$, $\mathbf{v} = (v_1,v_2,\dots,v_n)'$ with $v_i = 1$ if the $i^{\text{th}}$ subject failed and 0 otherwise. Letting $\mathbf{\lambda} = (\lambda_1,\lambda_2,\dots,\lambda_J)'$, we can write the likelihood function of $\lambda$ for the $n$ subjects as:

$$L(\lambda|D) = \prod_{i=1}^n \prod_{j=1}^J \lambda_j^{\delta_{ij}v_i} exp\bigg\{-\delta_{ij} \bigg[\lambda_j (y_i - \tau_{j-1}) + \sum_{g=1}^{j-1} \lambda_g(\tau_g - \tau_{g-1}) \bigg] \bigg\}$$
where $\delta_{ij} = 1$ if the $i^{\text{th}}$ subject failed or was censored in the  $j^{\text{th}}$ interval.


As a consequence of the constant hazards, the times within each interval are distributed as exponential random variables. The pdf of an exponential is:
$$\mathcal{E}(X|\lambda) = \lambda e^{-\theta X}, \quad 0 \leq X, 0 < \lambda$$

A convient prior for the exponential hazard ($\lambda$) is the gamma distribution. Because the exponential is a special case of the gamma distribution where the shape parameter is fixed at one, it is straightforward to prove that the gamma is congugate to the exponential.   

_Proof_
The gamma PDF is:
$$f(\lambda | \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\beta \lambda}$$
Suppose we now observe $x_1, x_2, x_n \sim \text{iid} \;  \mathcal{E}(X|\lambda)$ (exponential) and produce the likelihood function:


$$L(\lambda | \text{x}) = \prod_{i=1}^n \lambda e^{-\lambda x_i} = \lambda^n exp \bigg[-\lambda \sum_{i = 1}^n x_i \bigg] $$
Note that $\sum_{i=1}^n x_i$ is a sufficient statistic for $\lambda$. The posterior distribution is as follows:

\begin{align*}
\pi(\lambda| \text{x}) &\propto L(\lambda | \text{x})p(\lambda)\\
&= \lambda^n exp \bigg[ -\lambda \sum_{i=1}^n x_i \bigg]\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\beta \lambda}\\
&\propto \lambda^{(\alpha +n)-1} exp \bigg[ - \lambda \bigg( \sum_{i=1}^n x_i + \beta \bigg) \bigg]\\
\end{align*}

It is easy to see that this is the kernel of a $\text{Gamma}(\alpha + n , \sum x_i + \beta)$, and therefore the gamma distribution is shown to be conjugate to the exponential likelihood function. Therefore, conditional on the changepoints we can obtain a posterior sample of the hazard.


#Gibbs Sampler

##One-changepoint model

Because we can compute the conditonal distribution of the lambda's in each interval and  compute the likelihood across all potential changepoints (by restricting changepoints to be event times), we can use Gibbs sampling to obtain the marginal distribution for each variable by iteratively conditioning on interim values of the other parameters in a continuing cycle.

Assume the following model with independent priors

\begin{align*}
   k &\sim \text{Uniform} \{1,2,\cdots,n\}\\
   \lambda_{1} &\sim \text{Gamma}(\alpha_1, \beta_1)\\
   \lambda_{2} &\sim \text{Gamma}(\alpha_2, \beta_2)\\
\end{align*}

where $\alpha_1$, $\beta_1$, $\alpha_2$ and $\beta_2$ are independently distributed as a $\text{Gamma}(0.5, 5)$ random variables. These priors are non-informative as the mean of this distribution is 0.1 $E[X] = \frac{\alpha}{\beta}$.

Using the fact that the gamma distribution is conjugate prior to the exponential; we obtain the following:

\begin{align*}
\lambda_{1}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_1 +d_{[\tau_0 - \tau_1)}, \beta_1 + \sum_{i=1}^{[\tau_0 - \tau_1)} t)\\
\lambda_{2}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_2 +d_{[\tau_1 - \infty)}, \beta_2 + \sum_{i=1}^{[\tau_1 - \infty)} t)\\                  
\end{align*} 

where k is the location of the changepoint (represented in terms of the number of deaths that have occured), $\tau$ is time of the changepoint, d are the deaths in the interval and t is the exposure time for all observations within the interval. The posterior density of the changepoint $k$ is 

\begin{equation}
f(k|D,\lambda,\alpha, \beta) = \frac{L(D;k,\lambda)}{\sum_{j=1}^n L(D;j,\lambda)}
(\#eq:post-change)
\end{equation}

which is the likelihood of the piecewise exponential model with a changepoint at k, divided by all possible changepoint models.

The model proceeds as follows:

1. Initialize k by random draw from 2:(n-1) events
2. For each interation, indexed $m = 1,2, \dots$ repeat steps (3-9):
3. For the current value of k, define the number of events and the exposure time within each interval
4. Sample  $\lambda \sim  \text{Gamma}(\alpha + d, \beta + t)$ for each inteval where d is the number of events and t is the exposure time in the interval.
5. Sample $\alpha \; \& \; \beta \sim \text{Gamma}(0.01, \lambda + 0.01)$
6. Evaluate the likelihood for all potential changepoints from $2:n-1$ based on the updated lambda values
7. Generate  a new changepoint from the multinomial distribution defined by Equation 1 
8. Increment m


###Validation

From the code; I have validated that the various functions. I first validate that the function that is used to simulate piecewise exponential times works as expected Figure (\@ref(fig:piecewise-expo-haz)). 

**Not shown:** I then test that the loglikelihood of my "stripped" down version of phreg produces the same log-likelihood (at the MLE's). I then ensure that my other function which computes the log-likelihood for a given change point and hazard is correct. I do this by inputting the MLE hazards and obtaining the same log-likelihood as full phreg model. 

The algorithm seems to preform reasonably well, 500 iterations are completed in <4 mins with 100 discarded for burn-in. Figure \@ref(fig:gibbs-haz) presents the posterior samples of the hazards (in red), the mean of these posterior samples (black) and the "true" underlying hazards. The "true" hazards of 0.3 and 0.5 and timepoint of 3 years are recovered with some discrepancy expected because 300 exponential times were simulated and therefore the observed means maybe subject to sample variation. Table \@ref(tab:summary-tab) presents summary statistics for the changepont and hazards.  Figure \@ref(fig:piecewise-expo-surv) shows the Kaplan-Meier survival of the simulated observations versus the mean posterior survival (red). The mean posterior survival is a reasonable estimator of the Kaplan-Meier survival function and remains with the 95% confidence intervals across the timepoints.


```{r, include = FALSE}

setwd("C:/Users/phili/OneDrive/PhD/R codes")
source("Functions Gibbs Markdown.R")


###### Validation of piecewise exponential functions
n <- 300
rate <- c(0.2, .5,0.25)
ratemat <- matrix(rep(rate, n/2), nrow = n, 
                  ncol = 3, byrow = TRUE)
t1 <- 1.5
t2 <-3

t <- c(0, t1, t2) 
ptm <- proc.time()
samp <- hesim::rpwexp(n, ratemat, t)
proc.time() - ptm
summary(samp)
event <- rep(1,n)

plot(survfit(Surv(samp,event)~1))

samp <- samp[order(samp)]


df <- data.frame(time =samp, status =event , enter = 0)
res.out  <- exposure_death(df, changepoint = c(t1,t2))
res.out[,1]/res.out[,2]


#########

```



```{r, eval = FALSE, include = FALSE}

#Verify the piecewise exponential functions

df_test <- data.frame(time = samp, status = 1 , enter = 0)
df_test <- df_test[order(df_test$time),]


changepoint <- summary(df_test$time)["Median"]
piecewise_loglik(df_test, changepoint = changepoint)

fit <- phreg(Surv(enter, time, status) ~ 1, data = df_test, dist = "pch", cuts = changepoint)
fit$loglik[1]
fit$hazards
piecewise_loglik(df_test, changepoint = changepoint, method = "Not ML", lambda = fit$hazards)

```

```{r, cache = TRUE, results = 'hide'}

#Consider a 1 changepoint model
num.breaks <- 1
rate <- c(.3, 0.5)
# number of exponentially distributed observations
n <- 300
ratemat <- matrix(rep(rate, n/2), nrow = n,
                  ncol = 2, byrow = TRUE)

t <- c(0, 3)
samp <- rpwexp(n, ratemat, t)
event <- rep(1,n)

df <- data.frame(time =samp, status =event , enter = 0)
df <- df[order(df$time),]

#redunant for now but will be required when censored observations are included
event.df <- df[which(df$status == 1),]

n <- sum(df$status) #count number of events
m <- 500#length of the chain
lambda1 <- lambda2 <- changepoint <- k <- rep(NA,m) #Initialize lambda 1, lambda 2


k[1] <- sample(2:(n-1),1) #sample k 2:(n-1) 

#Hyperparameters 
a1 <- a2 <- b1 <- b2 <- 1

#run the Gibbs sampler 

for (i in 2:m) {
  kt <- k[i-1]
  changepoint[i] <- event.df[kt,"time"]
  #Indentify  number of events and exposure time for the given changepoint
  #I could use k directly as the number of events but this wouldn't generalize to more 
  #changepoints
  res_array <- exposure_death(df, changepoint = changepoint[i])
  #generate lambda1
  lambda1[i] <- rgamma(1, shape = a1 + res_array[1,1], rate = b1 + res_array[1,2])
  #generate lambda2
  lambda2[i] <- rgamma(1, shape = a2 + res_array[2,1], rate = b2 + res_array[2,2])
  #generate a1, a2, b1 and b2
  a1 <- b1 <- rgamma(1, shape = .01, rate = 0.01)
  a2 <- b2 <- rgamma(1, shape = .01, rate = 0.01)
  
  LL <-sapply(event.df[-c(1,n),1], FUN = piecewise_loglik, df = df, method = "Not ML" ,
             lambda = c(lambda1[i],lambda2[i]) )
  
  #Use Brobdingnag package because the numbers are so small that R doesn't compute them
  L <- exp(as.brob(LL))
  L <- as.numeric(L /Brobdingnag::sum(L))
  L <- L / sum(L)
  #generate k from discrete distribution L on 2:n-1
  k[i] <- sample(2:(n-1), prob=L, size=1)
}


```

```{r, include = FALSE}
burn_in <- 100
df_summary <- data.frame(changepoint = as.vector(changepoint[-c(1:burn_in)]),
                         lambda1 = lambda1[-c(1:burn_in)],
                         lambda2 = lambda2[-c(1:burn_in)])

rounded_df_summary <- round_df(df_summary, digits = 2)

```

```{r summary-tab, echo= FALSE, warning = FALSE}
knitr::kable(summary(rounded_df_summary), digits = 2, caption = 'Summary statistics for the changepoints and hazards') %>%
kable_styling(latex_options = c("striped", "hold_position"))

```

```{r, include = FALSE}
#end_time <- Sys.time()

#end_time - start_time

changepoint <- changepoint[-c(1:burn_in)]
initial.time <- rep(0, length(changepoint))
lambda1 <- lambda1[-c(1:burn_in)]
lambda2 <- lambda2[-c(1:burn_in)]


df.plot <- data.frame( timepoints =c(rbind( initial.time,changepoint, max(samp))),
                       hazards = c(rbind(lambda1,lambda2,lambda2)),
                       id = rep(1:length(changepoint),each =3))

df.mean <- data.frame( timepoints =c(rbind( initial.time,mean(changepoint), max(samp))),
                       hazards = c(rbind(mean(lambda1),mean(lambda2),mean(lambda2))),
                       id = rep(1:1,each =3))

df.true <- data.frame(timepoints = c(t, max(samp) ),
                      hazards = c(rate, rate[length(rate)]))
```

```{r gibbs-haz, echo = FALSE, results = 'hide', fig.cap=  'Posterior samples (red), Underlying hazard (green), Posterior mean hazard (black)', warning = FALSE}

ggplot(df.plot, aes(timepoints, hazards))+ 
  geom_step(aes(group = id), linetype = "dashed", alpha = 0.075, colour = "red")+
  geom_step(data = df.mean)+
  geom_step(data = df.true, colour = "green")+
  ylim(0, .65)
```


```{r, cache = FALSE, include = FALSE}

time <- seq(from = 0, to  = 12, by = 0.1)
t.len <- length(time)
St <- array(NA, dim = c(length(time),nrow(df_summary)))


for(i in 1:nrow(df_summary)){
  for (j in 1:t.len){
    if(time[j] < df_summary[i,1]){
      St[j,i] <-  exp(-df_summary[i,2]*time[j]) 
      
    }else{
      time.diff <- time[j] -df_summary[i,1]
      St[j,i] <- exp(-df_summary[i,1]*df_summary[i,2])*exp(-df_summary[i,3]*time.diff) 
    }
  
  }
}

KM.fit <- survfit(Surv(time,status)~1, data = df)
```


```{r piecewise-expo-surv, fig.cap = 'Kaplan Meier vs mean posterior survival (red)',echo = FALSE, results = 'hide', out.height= '80%', fig.asp= .75}
plot(KM.fit, xlab = "Time", ylab = "Survival probability")
lines(time, rowMeans(St), col = "red")
title("Survival plot (KM and mean predicted Survival)")
points(y = exp(-mean(df_summary[,1])*mean(df_summary[,2])),
       x = mean(df_summary[,1], na.rm = T), pch = 23, bg = "green")

```



##Multiple-changepoint model

It is theoretically straightforward to consider models with more than 1 changepoint. Letting n be the number of changepoints, k becomes the ordered vector $\mathbf{k} = (k_1,k_2,\dots,k_n)'$, while the hazards ($\lambda$) and hyperparameters ($\alpha$,$\beta$) become vectors with lenght n + 1. 

The model is intialized by samplying n changepoints (without replacement). The elements of the $\alpha$ & $\beta$ vectors are each independent draws from a $\text{Gamma}(0.5, 5)$ distribution.  

Based on the changepoints the elements of the $\lambda$ vector will be sampled as below. 

\begin{align*}
\lambda_{1}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_1 +d_{[\tau_0 - \tau_1)}, \beta_1 + \sum_{i=1}^{[\tau_0 - \tau_1)} t)\\
\lambda_{2}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_2 +d_{[\tau_1 - \infty)}, \beta_2 + \sum_{i=1}^{[\tau_1 - \tau_2)} t)\\ 
\ .\\
\ . \\
\lambda_{n+1}|\alpha, \beta, k &\sim \text{Gamma}(\alpha_{n+1} +d_{[\tau_{n} - \infty)}, \beta_{n+1} + \sum_{i=1}^{[\tau_{n} - \infty)} t)\\ 
\end{align*} 

The posterior density of the first changepoint is calculated by evaluating the model likelihood from changepoints $2$ to $k_{2-1}$ (refering to the last observation before changepoint 2) conditional on the the changepoint $k_2$ and $\lambda_2$ (See \@ref(eq:post-change)). Based on the newly sampled changepoint, the hazards are updated and posterior density of the second changepoint is calculated by evaluating the likelihood $k_{1+1}$ to $k_{3-1}$. The second changepoint is sampled from this posterior and the process continues until all the changepoints have been evaluated. 

A _DAG_ (directed acyclic graph) is presented in Figure \@ref(fig:DAG). For each changepoint (from 1:n) an $\alpha$, $\beta$ and changepoint location k are sampled. The number of events (d) and the exposure time for each interval (t) are determinstic functions of the changepoint location k and their relationship to k is denoted by black double arrows. This notation is consistent with illustrations used in [@Lunn.2013]. Note that in the multiple changepoint model $\alpha$, $\beta$ are derived from a $\text{Gamma}(0.01,0.01)$ distribution and are not sampled based on the previous hazard. This is done for computational simplicity and will have no impact on the results. 

```{r DAG, fig.cap = 'DAG for the multiple changepoint model', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

knitr::include_graphics("C:/Users/phili/OneDrive/PhD/Plots/DAG-multiple.png")
```

```{r, cache = TRUE, results = 'hide' }
#Consider a x changepoint model
num.breaks <- 2
rate <- c(.3, 0.5, 0.2)
# number of exponentially distributed observations
n <- 500
ratemat <- matrix(rep(rate, n/2), nrow = n,
                  ncol = num.breaks+1, byrow = TRUE)

t <- c(0, 3,5)
samp <- rpwexp(n, ratemat, t)
event <- rep(1,n)

df <- data.frame(time =samp, status =event , enter = 0)
df <- df[order(df$time),]

#redunant for now but will be required when censored observations are included
event.df <- unique(df[which(df$status == 1),])

n <- sum(df$status) #count number of unique event times
n.vec <- 1:n
m <- 500 #length of the chain
#Create vectors to store the changepoints and lambdas
changepoint <- k <- array(NA, dim = c(m, num.breaks)) 
lambda <- array(NA, dim = c(m, num.breaks+1))
k[1,] <- sample(2:(n-1), num.breaks, replace = FALSE) #sample k 2:(n-1) 
k[1,] <- k[1,][order(k[1,])]

#Initial alpha and beta Hyperparameters 
beta_array <- alpha_array <- array(rgamma(m*(num.breaks + 1), shape = 0.01, rate = 0.01),
                                   dim = c(m, (num.breaks+1)))
             
#run the Gibbs sampler 
for (i in 2:m) {
  kt <- k[i-1,]
  for(j in 1:(num.breaks)){
    
  changepoint[i,] <- event.df[kt,"time"]
  #Indentify  number of events and exposure time for the given changepoints
  res_array <- exposure_death(df, changepoint = changepoint[i,])
  #generate lambdas for all intervals
 
    for(q in 1:(num.breaks+1)){
      lambda[i,q] <-   rgamma(1, shape = alpha_array[i,q] + res_array[q,1], rate = beta_array[i,q] + res_array[q,2])} 
  
  #Compute the likelihood for the allowable changepoints
  
  #Get the interval of observations to test on 
  if(num.breaks == 1){
    evals <- n.vec[-c(1,n)]
    interval.df <- event.df[evals,1]
    eval_lambda <- 1:2
  }else if(num.breaks != 1 & j == num.breaks){ #final interval
    evals <- c((kt[j-1]+1):(n-1))
    interval.df <- cbind(event.df[kt[j-1],1],event.df[evals,1])
    eval_lambda <- (ncol(lambda)-2):ncol(lambda)
  }else if(num.breaks !=1 & j == 1){ #first interval
    evals <- c(2:(kt[j+1]-1))
    interval.df <- cbind(event.df[evals,1],event.df[kt[j+1],1])
    eval_lambda <- 1:3
  }else{ #middle interval 
    evals <- c((kt[j-1]+1):(kt[j+1]-1))
    interval.df <- cbind(event.df[kt[j-1],1],
                         event.df[evals,1],
                         event.df[kt[j+1],1])
    eval_lambda <- (j-1):(j+2)
  } 
  
  LL <-apply(interval.df,1,
               FUN = piecewise_loglik, df = df, method = "Not ML" ,
               lambda = lambda[i,eval_lambda]) 
  #Use Brobdingnag package because the numbers are so small that R doesn't compute them
  L <- exp(as.brob(LL))
  L <- as.numeric(L /Brobdingnag::sum(L))
  L <- L / sum(L)
  
  kt[j] <- sample(evals, prob=L, size=1)
  }
  
   k[i,] <- kt
   #Final evaluation of hazards
   for(q in 1:(num.breaks+1)){
     lambda[i,q] <-   rgamma(1, shape = alpha_array[i,q] + res_array[q,1], 
                             rate = beta_array[i,q] + res_array[q,2])} 
   
}
```

```{r, results = 'hide', echo = FALSE}
#Write the dataset to the working directory

write.xlsx(df, "C:/Users/phili/OneDrive/PhD/R codes/changepoint_df_markdown.xlsx")


```


```{r, echo = FALSE, results = 'hide'}

#Processing the results

burn_in <- 50
changepoint_names <- rep(NA, num.breaks)
lambda_names <- rep(NA, num.breaks+1)
for(i in 1:num.breaks){
  changepoint_names[i] <- paste0("changepoint_", i)
  } 
for(i in 1:(num.breaks+1)){
  lambda_names[i] <- paste0("lambda_", i)
} 
names_vector <- c(changepoint_names,lambda_names)

output_df <- cbind(changepoint, lambda)
output_df <- output_df[-c(1:burn_in),]
colnames(output_df) <- names_vector


```

```{r, echo = FALSE, results = 'hide'}

#Ploting the results

samp.plot <- df_hazard_plot(df = output_df, time.vec = samp, num.breaks = num.breaks)


df.summary <- df_hazard_plot(df = data.frame(t(apply(output_df,2,mean))),
               time.vec = quantile(samp,0.6), num.breaks = num.breaks)

ggplot(samp.plot, aes(timepoints, hazards))+ 
  geom_step(aes(group = id), linetype = "dashed", alpha = 0.075, colour = "red")+
  geom_step(data = df.summary)+
  ylim(c(0,1))+
  scale_x_continuous(breaks = seq(0, 40, by = 5))+
  annotate(geom="segment", x=seq(0,40,1), xend = seq(0,40,1),
           y=0, yend= 0.01)
rounded_output_df <- round_df(output_df,2)

```


```{r summary-tab-multi, echo= FALSE, warning = FALSE}
knitr::kable(summary(rounded_output_df), digits = 2, caption = 'Summary statistics for the changepoints and hazards') %>%
kable_styling(latex_options = c("striped", "hold_position"))

```


```{r, echo = FALSE, results = 'hide'}


time <- c(seq(from = 0, to  = max(samp), by = 0.1))
time.break <- df.summary[-c(1,nrow(df.summary)),
                         grep("timepoints", colnames(df.summary))]
t.len <- length(time)

St <- array(NA, dim = c(length(time),nrow(output_df)))
index <- grep("changepoint", colnames(output_df))

changepoint_df <- cbind(0,output_df[,index])
time.interval_df <- t(apply(changepoint_df,1,diff))

index2 <- grep("lambda", colnames(output_df))
lambda_df <- output_df[,index2]

cum_haz_df <- t(apply(time.interval_df*output_df[,head(index2,-1)], 1,cumsum))

surv_df <-  cbind(1,exp(-cum_haz_df))

break.points.Surv <- data.frame(time = time.break, Survival =  colMeans(surv_df[,-1]))

for(i in 1:nrow(output_df)){
  for (j in 1:t.len){
    if(max(which(time[j] >= changepoint_df[i,]))==1){
      St[j,i] <-  exp(-lambda_df[i,1]*time[j]) 
      
    }else{
      index3 <- max(which(time[j] >= changepoint_df[i,]))
      
      time.diff <- time[j] -changepoint_df[i,index3]
      St[j,i] <- surv_df[i,index3]*exp(-lambda_df[i,index3]*time.diff) 
    }
    
  }
}


St_mean <- data.frame(time = time, Survival = rowMeans(St))
result.km <- survfit(Surv(time,status)~1, data = df)

km.data <- data.frame(cbind(result.km[[c("time")]],result.km[[c("surv")]], result.km[[c("upper")]],result.km[[c("lower")]]))
colnames(km.data) <- c("time", "survival", "upper", "lower")


Surv.plot <- data.frame(Survival = c(unlist(St)), 
                      time = rep(time,nrow(output_df)),
                      id = rep(1:nrow(output_df), each = length(time)))


```

```{r, piecewise-expo-surv-mult, fig.cap = 'Kaplan Meier vs mean posterior survival (blue), average breakpoint (green)',echo = FALSE, results = 'hide', out.height= '80%', fig.asp= .75}
ggplot(data = Surv.plot,  aes(x = time, y = Survival))+
  geom_line(aes(group = id),size = 0.1, alpha = 0.025, colour = "red")+
  geom_line(data = St_mean, aes(x = time, y = Survival), colour = "blue")+
  geom_step(data = km.data, aes(x = time, y = survival), inherit.aes = F )+
  geom_step(data = km.data, aes(x = time, y = upper),linetype = "dashed", inherit.aes = F )+
  geom_step(data = km.data, aes(x = time, y = lower), linetype = "dashed", inherit.aes = F )+
  geom_point(data = break.points.Surv, aes(x = time, Survival), shape=23, fill = "green",inherit.aes = F)+
  scale_y_continuous(breaks = seq(0, 1, by = 0.1))+
  annotate(geom="segment", x=seq(0,40,1), xend = seq(0,40,1),
           y=0, yend= 0.01)

#https://github.com/janeshdev/r-dataanalysis-tips/wiki/Add-minor-ticks-to-ggplot2
```

#Computational Optimization

The algorithm presented above is computationally intensive due to the fact that the log-likelihood needs to be evaluated for each event time within the interval $k_{(X-1)+1}$ to $k_{(X+1)-1}$. Increasing the number of changepoints adds another changepoint which must evaluated conditional on the other changepoints. In models with a large number of observations and numerous changepoints (i.e. 4), evaluation of iteration is close to 1 second. One could reasonably expect that 5,000 - 10,000 simulations may be required to ensure convergence of the model (although this number may be conservative given that all proposal steps are accepted in a Gibbs sampler) which would diminish the practicality of this method.

The efficency of the method can be enhanced substantially by noting that there is no loss of information in recasting the data as _times between individual observations_ (See Figure \@ref(fig:Time-between-events)). Figure \@ref(fig:Time-between-events) provides the total time experienced by the sample between events with the number of points representing distinct observations. The contribution to log-likelihood for a particular interval is $D \text{log} \lambda - \lambda T$, where D is the number of events and T is the exposure time for that interval (i.e. sum of the events and exposure time are sufficient statistics for the log-likelihood of the piecewise exponential model). 

When the data is represented like this there is no need to resection the dataset based on the cut points, and the log-likelihood is a trivial summation. The resulting changes made the algorithm over 25 times faster; allowing for large number of simulations to be computed in a reasonable time frame. 

```{r, eval = FALSE, echo = FALSE}


num.breaks.plot <- 2
rate.plot <- c(.2, .5, 0.3)
# number of exponentially distributed observations
n_event.plot <- 10
ratemat.plot <- matrix(rep(rate.plot, n_event.plot/2), nrow = n_event.plot,
                  ncol = num.breaks.plot+1, byrow = TRUE)

time_break.plot <- c(0, 2, 4)
samp.plot <- hesim::rpwexp(n_event.plot, ratemat.plot, time_break.plot)
samp.plot <- samp.plot[order(samp.plot)]
event.plot <- rep(1,n_event.plot)
df_event.plot <- data.frame(time =samp.plot, status =event.plot , enter = 0)

plot(survfit(Surv(samp.plot, event.plot)~1), xlab = "Time", ylab = "Survival", main = "Kaplan-Meier Survival Plot")

df.plot <- df_event.plot


plot( y = df_recast(df_event.plot), x =df_event.plot$time, pch = 11, 
      yaxt="none", col = "blue", xlab = "Time of event" , ylab = "Cumulative Time between events", xlim = c(0,max(df_event.plot$time)))
text(1.5,7, expression(Sigma), col = "red")
text(2,7, "y = Total time", col = "red")
text(1.75,6, "Num. points = Events", col = "blue")
axis(2, seq(0,ceiling(max(df_recast(df_event.plot))),1),las=2, cex.axis=0.8, font=2,col.axis="red")
```



```{r Kaplan-Meier, fig.cap = 'Data as observed', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

knitr::include_graphics("C:/Users/phili/OneDrive/PhD/Plots/Observed events.PNG")
```


```{r Time-between-events, fig.cap = 'Data recast as time between events', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

knitr::include_graphics("C:/Users/phili/OneDrive/PhD/Plots/Time between events.PNG")
```


```{r Time-between-events, fig.cap = 'Data recast as time between events', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

knitr::include_graphics("C:/Users/phili/OneDrive/PhD/Plots/Time between events.png")
```

#Model Convergence

Model Convergence was assessed using the Rafferty diagnositic with the quantile (q) = 0.025 and probability (s) = 0.95 with the multiple margins of error (r) tested. If for example we assumed r to be 0.005 this corresponds to requiring that the cumulative distribution function of the 0.025 quantile be estimated to within $\pm$ 0.005 with probability 0.95. This would ensure that our 95% intervals have an actual posterior probability of between 0.94 and 0.96 (Raftery Lewis 1991). It should be noted that the estimates are conservative, so more iterations are suggested than necessary. The diagnostic also provides an assesment of the number of burn-in required.

Chain mixing was assessed by the gelman rubin diagnostic. Gelman and Rubin (1992) propose a general approach to monitoring convergence of MCMC output in which $m > 1$  parallel chains are run with starting values that are overdispersed relative to the posterior distribution. Convergence is diagnosed when the chains have "forgotten"" their initial values, and the output from all chains is indistinguishable. The gelman.diag diagnostic is applied to a single variable from the chain. It is based a comparison of within-chain and between-chain variances, and is similar to a classical analysis of variance.


```{r, cache = TRUE, echo = FALSE, warning = FALSE}

output <- gibbs.changepoint_chains_optim(df, num.breaks = 2, n.iter = 10000, burn_in = 0, num.chains = 4)

# Run with  5000 chains


```

The Raferty-Lewis diagnostic tests indicate that the in order to be confident that our 95% credible intervals lie between 94-96% of the true posterior probabilities 10,000 samples are required. If we are willing to reduce our accuracy threshold to 93-97% we may only require a maximum of 2,500 samples. The diagnositic test also indicates that the burnin period should be very small.


```{r, warning = FALSE}
raftery.diag(output[[1]]$chain_1, r = 0.005) 
raftery.diag(output[[1]]$chain_1, r = 0.01) 

```

The Gelman-Rubin statistic shows that there is good convergence between dispersed chains, even with moderate number of simulations. This is probably not too surprising given the short burin predicted by the Raferty-Lewis diagnostic and the fact that there are only a finite number of changepoints allowable.


```{r, echo = FALSE}
gelman.diag(output[[1]])
```

Geweke (1992) proposed a convergence diagnostic for Markov chains based on a test for equality of the means of the first and last part of a Markov chain (by default the first 10% and the last 50%). If the samples are drawn from the stationary distribution of the chain, the two means are equal and Geweke's statistic has an asymptotically standard normal distribution.

The test statistic is a standard Z-score: the difference between the two sample means divided by its estimated standard error. The standard error is estimated from the spectral density at zero and so takes into account any autocorrelation.

The Z-score is calculated under the assumption that the two parts of the chain are asymptotically independent. The Z-scores for each of the chains was less than +/-1.96 (and +/-1.68) indicating equality of means between the fractions. Furthermore, these results were obtained for chains in which no samples where discarded as burnin.  
```{r, echo = FALSE}
geweke.diag(output[[1]]$chain_1)

```

For the value of changepoint 1, mixing between the chain was also visually assessed and found to be non-divergent.

```{r, echo = FALSE}

col.names.req <- c("changepoint_1")
col.names.req2 <-unlist(lapply(col.names.req, FUN = function(x){paste0("^",x,"$")}))
index.col <- unlist(lapply(col.names.req2, 
                           FUN = function(x){grep(x,colnames(output[[1]][[1]]))}))

```


```{r mixing, fig.cap = 'Trace of changepoint 1', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

plot(output[[1]][,index.col])

```

```{r acf, fig.cap = 'Autocorrelation of changepoint 1', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

acf(output[[1]][,index.col][[1]], main = "") 

```

Based on this diagnostic tests conducted above, 10,000 simulations were run with only 100 samples discarded.


```{r, cache=TRUE, results = 'hide'}

output_final <- gibbs.changepoint_chains_optim(df, num.breaks = 2, n.iter = 10001, burn_in = 100, num.chains = 1)

```


The posterior distributions for the hazards are presented in Figure \@ref(fig:post-haz) with the posterior distribution for the changepoints in Figure \@ref(fig:post-change). 

```{r post-haz, fig.cap = 'Posterior hazards', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

plot(density(output_final[[1]][1]$chain_1[,3]),type="l",col="red", xlim = c(.0,.8), 
     main = "Posterior Distribution of \n hazards")
lines(density(output_final[[1]][1]$chain_1[,4]),type="l",col="blue")
lines(density(output_final[[1]][1]$chain_1[,5]),type="l",col="green")
legend(0.5, 20, legend=c("Hazard 1", "Hazard 2", "Hazard 3"),
       col=c("red", "blue", "green"), lty=1, cex=0.7)

```

```{r post-change,  fig.cap = 'Posterior distributions for changepoints', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

output_final[[5]]

```


#Model Validation 

It is possible to fit a piecewise-exponential using the BUGS/JAGS samplers (after a lot of fruitless attempts!). Implementation of the model requires use of the counting process formulation ([@Lunn.2013] See piecewise parameteric model). Due to the very large number of stochastic nodes > 80,000 the model requires 10 hours to evaluate 2,000 simulations in Winbugs (number of events is 500).


```{r, eval = FALSE}
#Processing of data
N = nrow(df)
T = length(unique(df$time))-1
eps=1.0E-10
obs.t = df$time
ind = df$status
t = unique(df$time)
#Jags model
start1 <-10 # We set uniform priors for the first interval across these ranges
end1 <- 90

start2 <-50 # Uniform prior for the second changepoint
end2 <- 150
p1 <- c(rep(0,start1),rep(1/(end1-start1),(end1-start1)),rep(0,(n-end1)))
p2 <- c(rep(0,start2),rep(1/(end2-start2),(end2-start2)),rep(0,(n-end2)))


piecewise.expo2 <- function(){
  
  for (i in 1:N) {
    for (j in 1:T){
      Y[i,j] <- step(obs.t[i] - t[j] + eps)
      dN[i,j] <- Y[i,j]*step(t[j+1] - obs.t[i] - eps)*ind[i]
    }
  }
  
  dt[1] <- t[1]
  for (j in 2:(T+1)) {
    dt[j] <- t[j] - t[j-1]
  }
  
  for (j in 1:T) {
    for (i in 1:N) {
      dN[i,j] ~ dpois(Idt[i,j])
      Idt[i,j] <- Y[i,j]*lam[period[j]]*dt[j]
    }
  }
  cumhaz.treat[1] <- 0
  
  for (j in 2:(T+1)) {
    
    cumhaz.treat[j] <- cumhaz.treat[j-1] + lam[period[j]]*dt[j]
    S.treat[j] <- exp(-cumhaz.treat[j])
  }
  for (j in 1:ndtimes) {
    lam[j] ~ dgamma(0.01, 0.01)
  }
  
  # Code to make the changepoints an unknown parameter
  
  for(i in 1:N){
    period[i]  <-  1 + step(i - k1 -0.01) + step(i - k2 -0.01)
  }
  
  k1 ~dcat(p1[ ])
  k2 ~dcat(p2[ ])
  
}



#Bugs Model

data <- list(N = n, T = T,eps=eps,
             obs.t = obs.t,
             ind = ind,
             t = t,
             ndtimes = num.breaks+1,
             p1 = p1, p2 = p2  )
inits <- function(){list(k1 = sample(50:200, 1),
                         k2 = sample(150:400, 1))}


```


```{r, echo = FALSE, warning = FALSE}

load(file = "Winbug2.RData")

lam1 <- sim$sims.array[,,"lam[1]"]
lam2 <- sim$sims.array[,,"lam[2]"]
lam3 <- sim$sims.array[,,"lam[3]"]

k1 <- sim$sims.array[,,"k1"]
k2 <- sim$sims.array[,,"k2"]

plot(sim$sims.array[,,"k1"])


#df <- read.xlsx("C:/Users/phili/OneDrive/PhD/R codes/changepoint_df_markdown.xlsx", 1)
time_df <- matrix(NA, nrow = 1000, ncol = 2)
time_df <- apply(cbind(k1,k2),c(1,2), function(x)df[x,"time"] )

change.point_df <- data.frame(changetime = c(apply(time_df,1, function(x)min(x)),
                       apply(time_df,1, function(x)max(x))), 
                       changepoint = c(rep(1,nrow(time_df)),                                                                             rep(2, nrow(time_df))))

change.point_df$changepoint <- factor(change.point_df$changepoint)
  
change.point_plot <- change.point_df %>% group_by(changepoint, changetime) %>%    dplyr::summarize(n = dplyr::n()) %>% mutate(perc = (n*100/nrow(time_df)))
  
plot_change  <- ggplot(change.point_plot, aes(x = changetime, y =perc, color=changepoint))+
    geom_pointrange(aes(ymin=0, ymax=perc), size = 0.02)+
    scale_x_continuous(name="Time", breaks = round(seq(round(min(change.point_plot$changetime),1),                                                   max(change.point_plot$changetime), by = 0.25),2) )+
    scale_y_continuous(name="Probability of Changepoint (%)")+
  ggtitle("Posterior distribution of changepoints")



```


The posterior distributions for the hazards are presented in Figure \@ref(fig:post-haz-BUGS) with the posterior distribution for the changepoints in Figure \@ref(fig:post-change-BUGS). Note that these are almost identical to the results obtained using the optimized Gibbs Sampler. 


```{r post-haz-BUGS, fig.cap = 'Posterior hazards', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}
plot(density(lam1),type="l",col="red", xlim = c(.0,.8), 
     main = "Posterior Distribution of \n hazards")
lines(density(lam2),type="l",col="blue")
lines(density(lam3),type="l",col="green")
legend(0.5, 20, legend=c("Hazard 1", "Hazard 2", "Hazard 3"),
       col=c("red", "blue", "green"), lty=1, cex=0.7)
```


```{r post-change-BUGS,  fig.cap = 'Posterior distributions for changepoints', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE}

plot_change

```



# Model performance with censoring

The model performance was tested in the presence of censoring. A variety of methods were tested to produce uniform censoring across the interval, however, only one produced uninformative censoring and that still censored more observations from the later periods than the earlier period. 

To achieve non-informative censoring a number of censored observations was selected and double this number of censorsed observations were generated using the same survival distribution as the events. For example if a model with 2 changepoints (at intervals 1 and 3 with hazards 0.2,0.4,0.2) was used to generate the observations the same model was used to generate the censors. Double the number of required censors were chosen, as on average only 50% of the censored observations would be accepted.


```{r, echo = FALSE, cache= TRUE, warning = TRUE}

#Consider a x changepoint model
num.breaks <- 2
rate <- c(.2, 0.4, 0.2)
# number of exponentially distributed observations
n <- 500
ratemat <- matrix(rep(rate, n/2), nrow = n,
                  ncol = num.breaks+1, byrow = TRUE)

t <- c(0, 1,3)
samp <- rpwexp(n, ratemat, t)
#If we want approx 150 censors to be evenly distributed across the time frame we should sample 
#300 censored observations from the same distribution (as only approx 50% will be greater than the 
#actual events)


max_time <- 5

n_cens_req <- 150
samp_cens <- rpwexp(n_cens_req*2, ratemat, t)
samp_cens <- sapply(samp_cens, FUN = min, max_time)
samp_cens <- sample(c(samp_cens,rep(max_time,n- n_cens_req*2))) # Randomized vector 
#http://www.cookbook-r.com/Manipulating_data/Randomizing_order/

df <- data.frame(time_event =samp, time_cens = samp_cens)
df$time <- apply(cbind(samp,samp_cens),1, min)
df <- df %>% dplyr::mutate(status = ifelse(samp <= samp_cens,1,0), enter = 0)




#plot(survfit(Surv(df$time, df$status)~1), xlim = c(0,8))

#plot(survfit(Surv(samp,rep(1,length(samp)))~1), xlim = c(0,8))

#survfit(Surv(df$time, df$status)~1)
#survfit(Surv(samp,rep(1,length(samp)))~1)
df2 <- df[,c("time","status")]
df2$obs <- "Cens"

df3 <- data.frame(time = df$"time_event")
df3$status <- 1 
df3$obs <- "True"

df4 <- rbind(df2,df3)


true_cens<- n - sum(df$status)

#plot(survfit(Surv(df$time,df$status)~1))

observed_events <- exposure_death(df[,c("time", "status", "enter")], t[-1] )

observed_haz <- observed_events[,1]/observed_events[,2]

#table(cut(df$time_event, breaks = c(t,Inf)))

```


In the example below I have considered `r n` observations with rate `r rate` and changepoints `r t[-1]`. I have also censored all observations above `r max_time` years. With these settings `r true_cens` observations are censored and the observed hazards withing the true intervals are `r observed_haz`. 


Figure \@ref(fig:cens-true) compares the true and censored observations. No biases are apparent. 

```{r  cens-true, fig.cap = 'Plot of the censored observations vs the true data', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE, echo = TRUE, cache=TRUE}
surv_true<- survfit(Surv(time,status)~obs, data = df4)
ggsurvplot(surv_true, risk.table = TRUE)
```

The models were fit to the censored data and their extrapolations compared against the true data.
Figure \@ref(fig:post-diff-surv) below compares the results for the different survival models against the true data. Although the graph lacks clarity, the piecewise exponential (purple) appears to quite closely follow the true distribution. (May need a statistical way of quantifying this).



```{r post-diff-surv, fig.cap = 'Plot of the different survival model versus the true data', echo = FALSE, out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE, echo = TRUE, cache=TRUE}



output_3 <- gibbs.changepoint_chains_optim(df[,c("time", "status")], num.breaks = 2, n.iter = 5000, burn_in = 100, num.chains = 1, max_predict = max(df$time_event),
                                           obs_true = df$time_event )


```


```{r summary-tab-multi2, echo= FALSE, warning = FALSE}
knitr::kable(summary(output_3[[4]]), digits = 2, caption = 'Summary statistics for the changepoints and hazards') %>%
kable_styling(latex_options = c("striped", "hold_position"))

```


```{r, echo = TRUE, cache=TRUE}

output_3[[3]]

```

As a test I will overspecify the number of changepoints in the model, by running a 3 changepoint model. Investigate the issue regarding high number of changeoints!


```{r echo = TRUE, cache=TRUE}



output_4 <- gibbs.changepoint_chains_optim(df[,c("time", "status")], num.breaks =10, n.iter = 10000, burn_in = 100, num.chains = 1, max_predict = max(df$time_event),
                                           obs_true = df$time_event )


```

```{r  echo= FALSE, warning = FALSE}
knitr::kable(summary(output_4[[4]]), digits = 2, caption = 'Summary statistics for the changepoints and hazards') %>%
kable_styling(latex_options = c("striped", "hold_position"))

```


```{r, echo = TRUE, cache=TRUE}

output_4[[3]]

```

#Model against real data


A three changepoint model was fit to the intervention (Interferon) arm of overall survival. 

```{r, echo =  FALSE, message=FALSE, results = 'hide', cache = TRUE}
#Initialize the packages if running from a Novartis Machine.
#Latex
#http://pages.stat.wisc.edu/~jgillett/371/RStudio/RMarkdown.pdf
#https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
#http://www.math.mcgill.ca/yyang/regression/RMarkdown/example.html
##https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols

url.path <- "http://merlot.stat.uconn.edu/~mhchen/survbook/dataset/e1690.missing.dat"
E1690.dat <- read.delim(url(url.path), header = T, sep="", skip=12, as.is=TRUE)
#Drop PFS events with time equal zero
E1690.dat <- E1690.dat[-which(E1690.dat$FAILTIME ==0),]

#Convert to the correct notation for survival objects
E1690.dat[which(E1690.dat$FAILCENS == 1),"FAILCENS"] <-0
E1690.dat[which(E1690.dat$FAILCENS == 2),"FAILCENS"] <-1
E1690.dat[which(E1690.dat$SURVCENS == 1),"SURVCENS"] <-0
E1690.dat[which(E1690.dat$SURVCENS == 2),"SURVCENS"] <-1



```



```{r, echo=FALSE,warning=FALSE, results='hide'}
#Need to turn the data into a Survival object, Event = 1 , censor = 0
source("Analysis functions.R")
#Create survival objects
fit.OS <- survfit(Surv(SURVTIME, SURVCENS)~TRT, 
                  data = E1690.dat[which(E1690.dat$STUDY == "1684"),])
fit.PFS <- survfit(Surv(FAILTIME, FAILCENS)~TRT, 
                   data = E1690.dat[which(E1690.dat$STUDY == "1684"),])

#Plot of Kaplan Meiers

OS.obs1 <- as.numeric(fit.OS$strata[1])
PFS.obs1 <- as.numeric(fit.PFS$strata[1])
```

```{r KM-E1684, fig.cap= 'PFS and OS outcomes in E1684 trial', out.width='80%', fig.asp=.75, fig.align='center',echo=FALSE,warning=FALSE, results='hide'}

# All treatment and outcomes plotted together
plot(fit.OS$time[1:OS.obs1],fit.OS$surv[1:OS.obs1],
     col = "red", lty = 1, type = "l", ylim = c(0,1), main = "OS and PFS Survival",
     xlab = "Time in years", ylab = "Survival")
lines(fit.OS$time[OS.obs1+1:length(fit.OS$time)],fit.OS$surv[OS.obs1+1:length(fit.OS$time)],
      col = "blue", lty = 1)
lines(fit.PFS$time[1:PFS.obs1],fit.PFS$surv[1:PFS.obs1],
      col = "darkred", lty = 2, type = "l")
lines(fit.PFS$time[PFS.obs1+1:length(fit.PFS$time)],fit.PFS$surv[PFS.obs1+1:length(fit.PFS$time)],
      col = "darkblue", lty = 2)
legend("topright", legend = c("OS Trt 1 (OBS)", "OS Trt 2 (INF)", "PFS Trt 1 (OBS)", "PFS Trt 2 (INF)"),
       col = c("red", "blue", "darkred", "darkblue"), lty = c(1,1,2,2), cex = 0.8)
```

```{r, echo = FALSE, cache = TRUE}
trt.arm <- E1690.dat[which(E1690.dat$TRT == 2),]

trt.arm <- trt.arm %>% dplyr::rename(time = SURVTIME, status = SURVCENS)

output_rwe <- gibbs.changepoint_chains_optim(trt.arm[,c("time", "status")], num.breaks = 7, n.iter = 10000, burn_in = 100, num.chains = 1, max_predict = 15)


```


```{r  echo= FALSE, warning = FALSE}
knitr::kable(summary(output_rwe[[4]]), digits = 2, caption = 'Summary statistics for the changepoints and hazards for real world dataset') %>%
kable_styling(latex_options = c("striped", "hold_position"))

```


```{r, echo = TRUE, cache=TRUE}

output_rwe[[3]]

```







#Not updated






#Model Uncertainty

## Evaluation of Pseudo-Marginal Likelihood

As seen in the previous section, Bayesian inference can evaluate the uncertainty in the location of changepoints for a model with a given number of changepoints (i.e. parameter uncertainty). A Bayesian framework can also be used to evaluate the uncertainty associated with the number of changepoints (i.e. model uncertainty). As discussed in [@Jackson.2010] the utility function $\boldsymbol U(\cdot)$ can be defined as the posterior predictive likelihood for $\boldsymbol y$, i.e. the likelihood integrated over the posterior distribution of the model parameters $\Theta$ as

$$\mathbf U_P(\mathbf y|\mathbf x,\mathbf M_k) = \int f(\mathbf y |\theta,\mathbf M_k ) \pi(\theta|\mathbf x,\mathbf M_k)d\theta$$

The expectation of this predictive utility for a replicate data set can be estimated, using only the sample data, by a cross-validatory predictive density termed PML.

$$f_P(\mathbf x|\mathbf M_k = \prod_if(x_i|))$$

It differs in aim from the marginal likelihood in expression (5 addin!), assessing predictive ability rather than fidelity to the data. Gefland adn Dey 1994 (add in reference) described an importance sampling method for estimating the PML based on a single MCMC model fit, which avoids the need to refit the model with each observation exclued in turn. (For ease of notation in this section, the dependence on the model $M_k$ is omitted.)

The full data posterio density $\pi (\theta| \mathbf x)$ is used as a proposal distribution to approximate the leave-one-out posterior density $\pi (\theta| \mathbf x_{(i)})$. Given an MCMC sample $\theta_1, \dots, \theta_N$ from the posterior of of $\theta$, the importance weights are then $w_{ir} = \pi(\theta_r| \mathbf x_{(i)})) / \pi(\theta_r| \mathbf x)  \propto 1/f(x_i | \theta_r)$, and the importance sampling estimate of $f(x_i | \mathbf x_{(i)})$ over the posterior sample:

\begin{align*}
f(x_i | \mathbf x_{(i)}) &\approx \sum_r w_{ir} f(x_i | \mathbf x_{(i)})/ \sum_r w_{ir} \\
&= N/ \sum_r \frac{1}{f(x_i | \theta_r)} \\
\end{align*}

#Calculating best predictive model

To avoid the computation expense of refitting models to calculate the model selection probabilities, the Bayesian bootstrap method described by Vehtari and Lampine (2002) was used. Instead of sampling with replacement from $\mathbf x$, the Bayesian bootstrap samples sets of probabilities $q_i$ that the random variable $X$ underlying the data takes the value of each sample point $x_i$. In one bootstrap iteration, samples $q_i^{(\text{rep})}$ of q_i are drawn from a "flat" Dirichlet distrution with all parameters 1. This is the psoterior distrution of _the disribution of X_, conditionally on the sample $\mathbf x$ and an improper prior (Rubin , 1981). The bootstrap replicate of the sample statistic is then computed by using the original data $\mathbf x$ with weights of $q_i^{(\text{rep})}$.

For the log(PML) example, the log-predictive-ordinates for each point $x_i$ is:

$$log{f_P(\mathbf x | M_k)} = \sum_{i= 1}^n log{f(x_i | \mathbf x_{(i)}, M_k)}$$ 

where $n$ is the sample size. The Bayesian bootstrap replicate of the log(PML) is then

$$log{f_P(\mathbf x | M_k)}^{\text(rep)} = n\sum_{i= 1}^n q_i^{\text(rep)}log{f(x_i | \mathbf x_{(i)}, M_k)}$$ 

```{r, eval = FALSE, cache = TRUE,echo = FALSE, results = 'hide', include = FALSE }

iters <- 2000
burn_in <- iters*.1

output.list_1 <- gibbs.changepoint_chains_optim(df = df, num.breaks = 1, n.iter = iters, burn_in = burn_in)
output.list_2 <- gibbs.changepoint_chains_optim(df = df, num.breaks = 2, n.iter = iters, burn_in = burn_in)
output.list_3 <- gibbs.changepoint_chains_optim(df = df, num.breaks = 3, n.iter = iters, burn_in = burn_in)
output.list_4 <- gibbs.changepoint_chains_optim(df = df, num.breaks = 4, n.iter = iters, burn_in = burn_in)

boot.samps <- 5000
alpha <- c(rep(1,nrow(df)))
weight_vec <- t(rdirichlet(boot.samps, alpha))

PML_1 <- PML.calc(origin.df = df, output_df = output.list_1[[4]],weights = weight_vec,
                  num.breaks = 1)
PML_2 <- PML.calc(origin.df = df, output_df = output.list_2[[4]], weights = weight_vec,
                  num.breaks = 2)
PML_3 <- PML.calc(origin.df = df, output_df = output.list_3[[4]], weights = weight_vec,
                  num.breaks = 3)
PML_4 <- PML.calc(origin.df = df, output_df = output.list_4[[4]], weights = weight_vec,
                  num.breaks = 4)


```




```{r, echo = TRUE, eval = FALSE}

round(table(apply(cbind(PML_1,PML_2,PML_3,PML_4),1, which.max))/boot.samps, digits = 2)

```
